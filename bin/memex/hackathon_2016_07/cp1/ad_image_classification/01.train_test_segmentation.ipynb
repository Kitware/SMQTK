{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Loading ad clusters and image SHA1s\n",
    "\n",
    "Loading cluster, ad and SHA1 ships from the base file.\n",
    "\n",
    "## Filter based on computed descriptors.\n",
    "Some images were not valid (i.e. were gifs or HTML pages), and should not be considered.\n",
    "This may cause some ads to no longer have child images and thus should also be not considered.\n",
    "This again applies to clusters that have no resulting child ads.\n",
    "\n",
    "Maps and files saved from this point on have been filtered by what has been actually computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import csv\n",
    "\n",
    "# SHA1 values of images actually computed for descriptors\n",
    "pos_computed_shas = {r[1] for r in csv.reader(open('positive.cmd.processed.csv'))}\n",
    "neg_computed_shas = {r[1] for r in csv.reader(open('negative.cmd.processed.csv'))}\n",
    "\n",
    "pos_cluster2ads = collections.defaultdict(set)\n",
    "pos_cluster2shas = collections.defaultdict(set)\n",
    "pos_ad2shas = collections.defaultdict(set)\n",
    "pos_sha2ads = collections.defaultdict(set)\n",
    "\n",
    "neg_cluster2ads = collections.defaultdict(set)\n",
    "neg_cluster2shas = collections.defaultdict(set)\n",
    "neg_ad2shas = collections.defaultdict(set)\n",
    "neg_sha2ads = collections.defaultdict(set)\n",
    "\n",
    "print \"Loading positive CSV file\"\n",
    "with open('./positive.CP1_clusters_ads_images.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for i, r in enumerate(reader):\n",
    "        if i == 0:\n",
    "            # skip header line\n",
    "            continue\n",
    "        c, ad, sha = r\n",
    "        c = int(c)\n",
    "        if sha in pos_computed_shas:\n",
    "            pos_cluster2ads[c].add(ad)\n",
    "            pos_cluster2shas[c].add(sha)\n",
    "            pos_ad2shas[ad].add(sha)\n",
    "            pos_sha2ads[sha].add(ad)\n",
    "        \n",
    "print \"Loading Negative CSV file\"\n",
    "with open('./negative.CP1_clusters_ads_images.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for i, r in enumerate(reader):\n",
    "        if i == 0:\n",
    "            # skip header line\n",
    "            continue\n",
    "        c, ad, sha = r\n",
    "        c = int(c)\n",
    "        if sha in neg_computed_shas:\n",
    "            neg_cluster2ads[c].add(ad)\n",
    "            neg_cluster2shas[c].add(sha)\n",
    "            neg_ad2shas[ad].add(sha)\n",
    "            neg_sha2ads[sha].add(ad)\n",
    "        \n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If negative cluster IDs intersect positive cluster IDs,\n",
    "# re-assign negative cluster IDs by increasing by max(pos_cluster_ids)\n",
    "pos_cluster_ids = set(pos_cluster2ads)\n",
    "neg_cluster_ids = set(neg_cluster2ads)\n",
    "if pos_cluster_ids & neg_cluster_ids:\n",
    "    print \"Reassigning cluster IDs\"\n",
    "    offset = max(pos_cluster_ids)\n",
    "    new_neg_cluster2ads  = collections.defaultdict(set)\n",
    "    new_neg_cluster2shas = collections.defaultdict(set)\n",
    "    \n",
    "    neg_cluster_id_old2new = {}\n",
    "    \n",
    "    for cid in sorted(neg_cluster_ids, reverse=True):\n",
    "        print \"- %d -> %d\" % (cid, cid+offset)\n",
    "        neg_cluster_id_old2new[cid] = cid+offset\n",
    "        \n",
    "        new_neg_cluster2ads[cid+offset] = neg_cluster2ads[cid]\n",
    "        new_neg_cluster2shas[cid+offset] = neg_cluster2shas[cid]\n",
    "    \n",
    "    neg_cluster_ids = set(new_neg_cluster2ad)\n",
    "    neg_cluster2ads = new_neg_cluster2ads\n",
    "    neg_cluster2shas = new_neg_cluster2shas\n",
    "    del new_neg_cluster2ads, new_neg_cluster2shas\n",
    "    \n",
    "    with open('negative.cluster_id_reassignment.old2new.pickle', 'w') as f:\n",
    "        print \"Saving reassignment mapping\"\n",
    "        cPickle.dump(neg_cluster_id_old2new, f, -1)\n",
    "    print \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SHA1's collected should now be <= to the SHA1s computed\n",
    "print len( {s for c, shas in pos_cluster2shas.iteritems() for s in shas}.difference(pos_computed_shas) )\n",
    "print len( {s for c, shas in neg_cluster2shas.iteritems() for s in shas}.difference(neg_computed_shas) )\n",
    "\n",
    "# Number of intersectring SHA1 between positive and negative set\n",
    "print len(set(pos_sha2ads) & set(neg_sha2ads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Saving clusters\n",
    "import json\n",
    "\n",
    "def convert_dict(a):\n",
    "    return dict( (k, list(v)) for k, v in a.iteritems() )\n",
    "\n",
    "json_params = {\"indent\": 2, \"separators\": (',', ': '), \"sort_keys\": True}\n",
    "\n",
    "\n",
    "# Saving positive info\n",
    "print \"Saving POS cluster->ads\"\n",
    "with open('positive.cluster2ads.pickle', 'w') as f:\n",
    "    cPickle.dump( pos_cluster2ads, f, -1 )\n",
    "\n",
    "print \"Saving POS cluster->image shas\"\n",
    "with open('positive.cluster2shas.pickle', 'w') as f:\n",
    "    cPickle.dump( pos_cluster2shas, f, -1 )\n",
    "\n",
    "print \"Saving POS ad->image shas\"\n",
    "with open('positive.ad2shas.pickle', 'w') as f:\n",
    "    cPickle.dump( pos_ad2shas, f, -1 )\n",
    "    \n",
    "print \"Saving POS SHA1->ads\"\n",
    "with open('positive.sha2ads.pickle', 'w') as f:\n",
    "    cPickle.dump( pos_sha2ads, f, -1 )\n",
    "\n",
    "\n",
    "# Saving negative info\n",
    "print \"Saving NEG cluster->ads\"\n",
    "with open('negative.cluster2ads.pickle', 'w') as f:\n",
    "    cPickle.dump( neg_cluster2ads, f, -1 )\n",
    "\n",
    "print \"Saving NEG cluster->image shas\"\n",
    "with open('negative.cluster2shas.pickle', 'w') as f:\n",
    "    cPickle.dump( neg_cluster2shas, f, -1 )\n",
    "\n",
    "print \"Saving NEG ad->image shas\"\n",
    "with open('negative.ad2shas.pickle', 'w') as f:\n",
    "    cPickle.dump( neg_ad2shas, f, -1 )\n",
    "\n",
    "print \"Saving NEG SHA1->ads\"\n",
    "with open('negative.sha2ads.pickle', 'w') as f:\n",
    "    cPickle.dump( neg_sha2ads, f, -1 )\n",
    "\n",
    "\n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Train and Test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating data based on Clusters\n",
    "\n",
    "Based on ordering clusters by the total number of child images.  This lets us base train/test sets of approximately the same relative sizes.\n",
    "\n",
    "Train 1 has so many images because one cluster has ~30k child images (cluster 417).\n",
    "\n",
    "We create 3 \"train\" sets:\n",
    "  - **`train1`**: one for training the image classifier\n",
    "  - **`train2`**: one for applying the image classifier and training the ad classifier\n",
    "  - **`train3`**: one for applying the image+ad classifier and training the cluster classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_clusters_ordered = sorted( pos_cluster2shas, \n",
    "                               key=lambda c: ( len(pos_cluster2shas[c]), c ),\n",
    "                               reverse=1 )\n",
    "neg_clusters_ordered = sorted( neg_cluster2shas,\n",
    "                               key=lambda c: ( len(neg_cluster2shas[c]), c ),\n",
    "                               reverse=1)\n",
    "\n",
    "# Image classifier training clusters/ads/shas\n",
    "train1_pos_clusters = { c   for i, c in enumerate(pos_clusters_ordered) if i % 4 == 0 }\n",
    "train1_neg_clusters = { c   for i, c in enumerate(neg_clusters_ordered) if i % 4 == 0 }\n",
    "train1_pos_ads      = { ad  for c in train1_pos_clusters for ad  in pos_cluster2ads[c] }\n",
    "train1_neg_ads      = { ad  for c in train1_neg_clusters for ad  in neg_cluster2ads[c] }\n",
    "train1_pos_shas     = { sha for c in train1_pos_clusters for sha in pos_cluster2shas[c] }\n",
    "train1_neg_shas     = { sha for c in train1_neg_clusters for sha in neg_cluster2shas[c] }\n",
    "\n",
    "# Ad histogram classifier training clusters/ads/shas\n",
    "train2_pos_clusters = { c for i, c in enumerate(pos_clusters_ordered) if i % 4 == 1 }\n",
    "train2_neg_clusters = { c for i, c in enumerate(neg_clusters_ordered) if i % 4 == 1 }\n",
    "train2_pos_ads      = { ad  for c in train2_pos_clusters for ad  in pos_cluster2ads[c] }\n",
    "train2_neg_ads      = { ad  for c in train2_neg_clusters for ad  in neg_cluster2ads[c] }\n",
    "train2_pos_shas     = { sha for c in train2_pos_clusters for sha in pos_cluster2shas[c] }\n",
    "train2_neg_shas     = { sha for c in train2_neg_clusters for sha in neg_cluster2shas[c] }\n",
    "\n",
    "# Cluster histogram classifier training clusters/ads/shas\n",
    "train3_pos_clusters = { c for i, c in enumerate(pos_clusters_ordered) if i % 4 == 2 }\n",
    "train3_neg_clusters = { c for i, c in enumerate(neg_clusters_ordered) if i % 4 == 2 }\n",
    "train3_pos_ads      = { ad  for c in train3_pos_clusters for ad  in pos_cluster2ads[c] }\n",
    "train3_neg_ads      = { ad  for c in train3_neg_clusters for ad  in neg_cluster2ads[c] }\n",
    "train3_pos_shas     = { sha for c in train3_pos_clusters for sha in pos_cluster2shas[c] }\n",
    "train3_neg_shas     = { sha for c in train3_neg_clusters for sha in neg_cluster2shas[c] }\n",
    "\n",
    "# Test/Validation clusters/ads/shas\n",
    "test_pos_clusters   = { c for i, c in enumerate(pos_clusters_ordered) if i % 4 == 3 }\n",
    "test_neg_clusters   = { c for i, c in enumerate(neg_clusters_ordered) if i % 4 == 3 }\n",
    "test_pos_ads        = { ad  for c in test_pos_clusters for ad  in pos_cluster2ads[c] }\n",
    "test_neg_ads        = { ad  for c in test_neg_clusters for ad  in neg_cluster2ads[c] }\n",
    "test_pos_shas       = { sha for c in test_pos_clusters for sha in pos_cluster2shas[c] }\n",
    "test_neg_shas       = { sha for c in test_neg_clusters for sha in neg_cluster2shas[c] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Train 1 (image)\"\n",
    "print \"  (pos)| clusters:\", len(train1_pos_clusters)\n",
    "print \"       | ads:\",      len(train1_pos_ads)\n",
    "print \"       | images:\",   len(train1_pos_shas)\n",
    "print\n",
    "print \"  (neg)| clusters:\", len(train1_neg_clusters)\n",
    "print \"       | ads:\",      len(train1_neg_ads)\n",
    "print \"       | images:\",   len(train1_neg_shas)\n",
    "print\n",
    "print \"Train 2 (ads)\"\n",
    "print \"  (pos)| clusters:\", len(train2_pos_clusters)\n",
    "print \"       | ads:\",      len(train2_pos_ads)\n",
    "print \"       | images:\",   len(train2_pos_shas)\n",
    "print\n",
    "print \"  (neg)| clusters:\", len(train2_neg_clusters)\n",
    "print \"       | ads:\",      len(train2_neg_ads)\n",
    "print \"       | images:\",   len(train2_neg_shas)\n",
    "print\n",
    "print \"Train 3 (cluster)\"\n",
    "print \"  (pos)| clusters:\", len(train3_pos_clusters)\n",
    "print \"       | ads:\",      len(train3_pos_ads)\n",
    "print \"       | images:\",   len(train3_pos_shas)\n",
    "print\n",
    "print \"  (neg)| clusters:\", len(train3_neg_clusters)\n",
    "print \"       | ads:\",      len(train3_neg_ads)\n",
    "print \"       | images:\",   len(train3_neg_shas)\n",
    "print\n",
    "print \"Test\"\n",
    "print \"  (pos)| clusters:\", len(test_pos_clusters)\n",
    "print \"       | ads:\",      len(test_pos_ads)\n",
    "print \"       | images:\",   len(test_pos_shas)\n",
    "print\n",
    "print \"  (neg)| clusters:\", len(test_neg_clusters)\n",
    "print \"       | ads:\",      len(test_neg_ads)\n",
    "print \"       | images:\",   len(test_neg_shas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train1 - for image classifier\n",
    "cPickle.dump(train1_pos_clusters, open('train1_pos_clusters.pickle', 'w'), -1)\n",
    "cPickle.dump(train1_pos_ads,      open('train1_pos_ads.pickle', 'w'), -1)\n",
    "cPickle.dump(train1_pos_shas,     open('train1_pos_shas.pickle', 'w'), -1)\n",
    "\n",
    "cPickle.dump(train1_neg_clusters, open('train1_neg_clusters.pickle', 'w'), -1)\n",
    "cPickle.dump(train1_neg_ads,      open('train1_neg_ads.pickle', 'w'), -1)\n",
    "cPickle.dump(train1_neg_shas,     open('train1_neg_shas.pickle', 'w'), -1)\n",
    "\n",
    "# Train2 - for ad histogram classifier\n",
    "cPickle.dump(train2_pos_clusters, open('train2_pos_clusters.pickle', 'w'), -1)\n",
    "cPickle.dump(train2_pos_ads,      open('train2_pos_ads.pickle', 'w'), -1)\n",
    "cPickle.dump(train2_pos_shas,     open('train2_pos_shas.pickle', 'w'), -1)\n",
    "\n",
    "cPickle.dump(train2_neg_clusters, open('train2_neg_clusters.pickle', 'w'), -1)\n",
    "cPickle.dump(train2_neg_ads,      open('train2_neg_ads.pickle', 'w'), -1)\n",
    "cPickle.dump(train2_neg_shas,     open('train2_neg_shas.pickle', 'w'), -1)\n",
    "\n",
    "# Train3 - for cluster histogram classifier\n",
    "cPickle.dump(train3_pos_clusters, open('train3_pos_clusters.pickle', 'w'), -1)\n",
    "cPickle.dump(train3_pos_ads,      open('train3_pos_ads.pickle', 'w'), -1)\n",
    "cPickle.dump(train3_pos_shas,     open('train3_pos_shas.pickle', 'w'), -1)\n",
    "\n",
    "cPickle.dump(train3_neg_clusters, open('train3_neg_clusters.pickle', 'w'), -1)\n",
    "cPickle.dump(train3_neg_ads,      open('train3_neg_ads.pickle', 'w'), -1)\n",
    "cPickle.dump(train3_neg_shas,     open('train3_neg_shas.pickle', 'w'), -1)\n",
    "\n",
    "# Test - for image/ad/cluster classifier validation\n",
    "cPickle.dump(test_pos_clusters, open('test_pos_clusters.pickle', 'w'), -1)\n",
    "cPickle.dump(test_pos_ads,      open('test_pos_ads.pickle', 'w'), -1)\n",
    "cPickle.dump(test_pos_shas,     open('test_pos_shas.pickle', 'w'), -1)\n",
    "\n",
    "cPickle.dump(test_neg_clusters, open('test_neg_clusters.pickle', 'w'), -1)\n",
    "cPickle.dump(test_neg_ads,      open('test_neg_ads.pickle', 'w'), -1)\n",
    "cPickle.dump(test_neg_shas,     open('test_neg_shas.pickle', 'w'), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating grount-truth json-lines file for test-set for use with MEMEX-provided evaluation script\n",
    "# format: {\"cluster_id\": \"<number>\", \"class\": <int>}\n",
    "# Class value should be:\n",
    "# - 1 for positive\n",
    "# - 0 for negative\n",
    "with open('test_eval_gt.jl', 'w') as f:\n",
    "    for c in sorted(pos_cluster_ids):\n",
    "        f.write( json.dumps({\"cluster_id\": str(c), \"class\": 1}) + \"\\n\" )\n",
    "    for c in sorted(neg_cluster_ids):\n",
    "        f.write( json.dumps({\"cluster_id\": str(c), \"class\": 0}) + \"\\n\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHA1 Intersection Investigation\n",
    "\n",
    "Some images have been found to be shared across ads in different clusters.  Since clusters are supposed to represent distictly seperate entities or relationships, this shows that either the clusters are not linkable via multimedia only, they were incorrectly clustered, or actively split up on purpose.  For the purpose of our approach (image-base classification), this means that the same images will potentially show up in both or all of the train/test/evaluation data sets.\n",
    "\n",
    "Traditionally, the presence of the same/similar images in both train and test sets leads to faulty evaluation because the classifier has an easier time handling data it was trained on, and thus artificially higher scores.  This is the same here in that train/test scores will probably be higher with their presence on both sides.  However, if their shared presence is a strong positive indicator of a new HT ad, so their repeated positive recognition is a boon.  On the other hand, again, we may want to see/measure how the classifier is performing due to abstract features, not including repeat imagery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_1_2_intersection = train1_pos_shas & train2_pos_shas\n",
    "train_2_3_intersection = train2_pos_shas & train3_pos_shas\n",
    "train_test_intersection = (train1_pos_shas | train2_pos_shas | train3_pos_shas) & test_pos_shas\n",
    "\n",
    "print len(train_1_2_intersection)\n",
    "print len(train_2_3_intersection)\n",
    "print len(train_test_intersection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Image intersection observations\n",
    "\n",
    "Found non-trivial images (i.e. not website logos or similar) in separate clusters:\n",
    "\n",
    "    train_1_shas & train_2_shas & test_shas\n",
    "    \n",
    "For example, the image with hash ``372a91ac487a27554d0017c48f66facbaa9a8f19`` is shared between positive clusters 348, 690, 673, and 148. Visual verification of the images in the ads that image is a part of shows what is definitely the same person.  There are other images in ads that are near duplicates to images in other ads."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

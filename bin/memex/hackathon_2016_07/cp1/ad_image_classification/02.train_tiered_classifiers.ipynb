{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiered/Multi-level Classifier training\n",
    "\n",
    "We assume here that the notebook `01.train_test_segmentation.ipynb` has been run and outputs generated.\n",
    "\n",
    "## What are we doing\n",
    "The idea of this approach is to train multiple levels of classifiers based on lower level classifier results.\n",
    "\n",
    "With regards to just images, this means that we will train an solely-image-based classifer that will attempt to classify single images as either HT or non-HT (HT = human trafficing releated).  We then create a histogram per ad based on the real-values results of child image classification results (probabilities).  The histogram generated will have a number of bins based on an arbitrary partitioning of the classification probability space ([0,1] range, we'll pick the arbitrary bin size later).  After ads have been assigned histograms, we will train a second classifier using the histograms as the descriptors.  Above ads, we have \n",
    "\n",
    "The previous notebook in this series notes how we must split train/test/evaluation sets based on the highest organizational level.  In our case here, that would be the cluster level.  In addition, for each level, we will have to split the descriptor data in half for classifier training and application in order to eliminate train/test bias.\n",
    "\n",
    "For our example where the final goal is a classifier for cluster-level histograms, we must split the training images (positive and negative) in half, based on cluster separation (highest order organization), where the first half is used to train the classifier, and the classifier is applied to the second half in order to create histograms for ads.  Of the result ad histograms, we must split that in half again for in order to train the ad histogram classifier on half, and then apply the classifier on the other half of ad histograms in order to construct the cluster-level histogram classifier.  If the final goal is just an ad histogram classifier (like before we received cluster-separated negative training data), then splitting the ads in half would not be necessary and training of the ad classifier could use all available ad histograms from the training data.\n",
    "\n",
    "Later on we will also note where it is possible to fuse in results from different classifiers to create a potentially more robust higher level result, which is the advantage of this approach over others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets import things and enable logging\n",
    "import collections\n",
    "import logging\n",
    "from smqtk.utils.bin_utils import initialize_logging\n",
    "initialize_logging(logging.getLogger('smqtk'), logging.DEBUG)\n",
    "initialize_logging(logging.getLogger(__name__), logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training level 1 - Image classifier\n",
    "\n",
    "Let us use the `train1...` partition to train the image classifier.\n",
    "We will use the recorded SHA1 values to fetch computed descriptors to use for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Loading positive ads/shas\"\n",
    "train1_pos_shas = cPickle.load(open('train1_pos_shas.pickle'))\n",
    "print \"Loading negative ads/shas\"\n",
    "train1_neg_shas = cPickle.load(open('train1_neg_shas.pickle'))\n",
    "\n",
    "print \"Done\"\n",
    "\n",
    "# Creating training CSV file for use with ``classifier_model_validation.py`` for classifier training.\n",
    "from smqtk.representation.descriptor_index.postgres import PostgresDescriptorIndex\n",
    "image_descr_index = PostgresDescriptorIndex(table_name='descriptor_index_alexnet_fc7',\n",
    "                                            db_host='/dev/shm',\n",
    "                                            db_port=5434,\n",
    "                                            db_user='purg',\n",
    "                                            multiquery_batch_size=10000,\n",
    "                                            read_only=True)\n",
    "\n",
    "print \"Descriptors stored:\", image_descr_index.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is, relatively for SVMs, a lot of training data, we will need to sub-sample positive and negative pools into smaller sets so we can allow the SVM to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from smqtk.algorithms.classifier.libsvm import LibSvmClassifier\n",
    "from smqtk.representation.classification_element.memory import MemoryClassificationElement\n",
    "from smqtk.representation.classification_element.file import FileClassificationElement\n",
    "from smqtk.representation import ClassificationElementFactory\n",
    "\n",
    "image_classifier = LibSvmClassifier('image_classifier.train1.classifier.model',\n",
    "                                    'image_classifier.train1.classifier.label',\n",
    "                                    normalize=2)\n",
    "c_file_factory = ClassificationElementFactory(FileClassificationElement, {\n",
    "        \"save_dir\": \"image_classifier.classifications\",\n",
    "        \"subdir_split\": 10,\n",
    "    })\n",
    "\n",
    "# Increase these numbers for final model training?\n",
    "# n_pos = 5000\n",
    "# n_neg = 10000\n",
    "n_pos = 3000\n",
    "n_neg = 6000\n",
    "\n",
    "random.seed(0)\n",
    "train1_pos_shas_rand = sorted(train1_pos_shas)\n",
    "train1_neg_shas_rand = sorted(train1_neg_shas)\n",
    "random.shuffle(train1_pos_shas_rand)\n",
    "random.shuffle(train1_neg_shas_rand)\n",
    "train1_pos_shas_rand = train1_pos_shas_rand[:n_pos]\n",
    "train1_neg_shas_rand = train1_neg_shas_rand[:n_neg]\n",
    "\n",
    "# Writing out as CSV file for posterity\n",
    "with open('image_classifier.train1.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for sha in train1_pos_shas_rand:\n",
    "        writer.writerow([sha, 'positive'])\n",
    "    for sha in train1_neg_shas_rand:\n",
    "        writer.writerow([sha, 'negative'])\n",
    "\n",
    "# Get the descriptors from the image index for the selected SHA1 values\n",
    "train1_pos_descrs = set(image_descr_index.get_many_descriptors(train1_pos_shas_rand))\n",
    "train1_neg_descrs = set(image_descr_index.get_many_descriptors(train1_neg_shas_rand))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Do training if we haven't yet\n",
    "if not image_classifier.has_model():\n",
    "    image_classifier.train(positive=train1_pos_descrs, negative=train1_neg_descrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating CSV file of train2 sha/truth values for image classifier validation\n",
    "IMG_MODEL_VALIDATION_TRUTH_CSV = \"image_classifier.train2_validation.csv\"\n",
    "\n",
    "print \"loading train2 pos shas\" \n",
    "train2_pos_shas = cPickle.load(open('train2_pos_shas.pickle'))\n",
    "\n",
    "print \"loading neg train2 shas\"\n",
    "train2_neg_shas = cPickle.load(open('train2_neg_shas.pickle'))\n",
    "\n",
    "print \"Writing CSV file\"\n",
    "with open(IMG_MODEL_VALIDATION_TRUTH_CSV, 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for sha in train2_pos_shas:\n",
    "        writer.writerow([sha, 'positive'])\n",
    "    for sha in train2_neg_shas:\n",
    "        writer.writerow([sha, 'negative'])\n",
    "\n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can optionally run the following for model validation using the Train2 data:\n",
    "\n",
    "    classifier_model_validation.py -vc image_classifier.train2_validation.json 2>&1 | tee image_classifier.train2_validation.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Ad histograms\n",
    "\n",
    "Steps:\n",
    "- Compute image probabilities using image classifier\n",
    "- collect image probabilities per ad\n",
    "- create N-element histogram of child image probabilities per add\n",
    "- train classifier using histograms (Validate using train3?)\n",
    "\n",
    "### Applying the image classifier\n",
    "Now that we have an image classifier, we need to apply that classifier to the second half of training images (this might take a while)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TRAIN2_IMG_CLASSIFICATIONS_CACHE = 'image_classifier.train2.classifications_cache.pickle'\n",
    "\n",
    "train2_pos_ads  = cPickle.load(open('train2_pos_ads.pickle'))\n",
    "train2_neg_ads  = cPickle.load(open('train2_neg_ads.pickle'))\n",
    "\n",
    "if os.path.isfile(TRAIN2_IMG_CLASSIFICATIONS_CACHE):\n",
    "    print \"Loading existing train2 image classification results\"\n",
    "    train2_img_descr2classifications = cPickle.load(open(TRAIN2_IMG_CLASSIFICATIONS_CACHE))\n",
    "    \n",
    "else:\n",
    "    # Apply classifier to image descriptors, collecting positive confidence\n",
    "    shas_to_compute = train2_pos_shas | train2_neg_shas\n",
    "    print \"Classifying positive/negative train2 image descriptors (count: %d)\" % len(shas_to_compute)\n",
    "    \n",
    "    train2_img_descr2classifications = \\\n",
    "        image_classifier.classify_async(image_descr_index.get_many_descriptors(shas_to_compute), \n",
    "                                        c_file_factory, \n",
    "                                        use_multiprocessing=True,\n",
    "                                        ri=1.0)\n",
    "\n",
    "    # Saving out results\n",
    "    with open(TRAIN2_IMG_CLASSIFICATIONS_CACHE, 'w') as f:\n",
    "        cPickle.dump(train2_img_descr2classifications, f, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_ad2shas = cPickle.load(open('positive.ad2shas.pickle'))\n",
    "pos_sha2ads = cPickle.load(open('positive.sha2ads.pickle'))\n",
    "neg_ad2shas = cPickle.load(open('negative.ad2shas.pickle'))\n",
    "neg_sha2ads = cPickle.load(open('negative.sha2ads.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "\n",
    "def filter_sha2ads(sha_set, ad_set):\n",
    "    \"\"\"\n",
    "    Create a SHA1 to ads mapping from the master sets, filtered on the given \n",
    "    SHA1 and ad subsets.\n",
    "    \n",
    "    :param sha_set: Sub-set of SHA1 values to consider\n",
    "    :param ad:set: Sub-set of ad IDs to consider.\n",
    "    :return: New mapping of SHA1 values to parent ads using only subsets given.\n",
    "    \"\"\"\n",
    "    new_sha2ads = collections.defaultdict(set)\n",
    "    for sha in sha_set:\n",
    "        # Get all ads associated with the SHA1, then filter with given ad set\n",
    "        new_sha2ads[sha] = (pos_sha2ads[sha] | neg_sha2ads[sha]) & ad_set\n",
    "    return new_sha2ads\n",
    "\n",
    "\n",
    "def make_histogram(probs, n_bins):\n",
    "    \"\"\" Make histogram of the given positive probability values \"\"\"\n",
    "    return numpy.histogram(probs, bins=n_bins, range=[0,1])[0]\n",
    "\n",
    "\n",
    "def generate_ad_histograms(img_classifications, sha2ads, hist_size=20, target_class='positive'):\n",
    "    \"\"\"\n",
    "    Generate ad histograms based on input iterable of ClassificationElements.\n",
    "    \n",
    "    This first aggregates image classification `target_class` scores under parent ads,\n",
    "    and then creates a histogram of `hist_size` bins for each ad based on the image\n",
    "    classification for images associated with that ad.  Histograms are normalized to\n",
    "    relative frequencies (L1 nomalization).\n",
    "    \n",
    "    :param img_classifications: Iterable of ClassificationElement instances for all images\n",
    "        classified.\n",
    "    :param sha2ads: Mapping of image descriptor UUID values (image SHA1) to the one or more\n",
    "        ads that image is a child of.\n",
    "    :param hist_size: The number of percentile partitions to make for the ad histograms.\n",
    "        This is literally the size of the histograms output.\n",
    "    :param target_class: The class probability to extract from input ClassificationElement\n",
    "        instances.\n",
    "    :return: Dict mapping of ad ID to the its L1 normalized histogram of child image\n",
    "        classification scores.\n",
    "    \"\"\"\n",
    "    ad2img_probs = collections.defaultdict(dict)\n",
    "    for c in img_classifications:\n",
    "        sha = c.uuid\n",
    "        for ad in sha2ads[sha]:\n",
    "            ad2img_probs[ad][sha] = c[target_class]\n",
    "            \n",
    "    ad2histogram = {}\n",
    "    for ad, img_prob_map in ad2img_probs.iteritems():\n",
    "        h = make_histogram(img_prob_map.values(), hist_size).astype(float)\n",
    "        if not h.sum():\n",
    "            raise RuntimeError(\"Zero-sum histogram for ad: %s (probs: %s)\" \n",
    "                               % (ad, img_prob_map.values()))\n",
    "        h /= h.sum()\n",
    "        ad2histogram[ad] = h\n",
    "    \n",
    "    return ad2histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reverse mapping within the scope of train2 only\n",
    "train2_sha2ads = filter_sha2ads(train2_pos_shas | train2_neg_shas,\n",
    "                                train2_pos_ads | train2_neg_ads)\n",
    "assert len(train2_img_descr2classifications) == len(train2_sha2ads), \\\n",
    "    \"The same number of SHA1 values in reverse map should equal the classifications computed\"\n",
    "\n",
    "train2_ad2histogram = generate_ad_histograms(train2_img_descr2classifications.itervalues(), \n",
    "                                             train2_sha2ads)\n",
    "\n",
    "with open('ad_classifier.train2_ad2histogram.pickle', 'w') as f:\n",
    "    cPickle.dump(train2_ad2histogram, f, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from smqtk.representation import DescriptorElementFactory\n",
    "from smqtk.representation.descriptor_element.local_elements import DescriptorMemoryElement\n",
    "from smqtk.representation.descriptor_index.memory import MemoryDescriptorIndex\n",
    "\n",
    "ad_histogram_descriptor_index = \\\n",
    "    MemoryDescriptorIndex(file_cache='ad_classifier.ad_histogram_descriptor_index.pickle')\n",
    "d_mem_factory = DescriptorElementFactory(DescriptorMemoryElement, {})\n",
    "    \n",
    "to_add = set()\n",
    "\n",
    "for ad in train2_ad2histogram:\n",
    "    d = d_mem_factory('ad-histogram', ad)\n",
    "    d.set_vector(train2_ad2histogram[ad])\n",
    "    if not ad_histogram_descriptor_index.has_descriptor(d.uuid()):\n",
    "        to_add.add(d)\n",
    "print \"Adding %d missing ad histograms\" % len(to_add)\n",
    "ad_histogram_descriptor_index.add_many_descriptors(to_add)\n",
    "del to_add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ad histogram classifier\n",
    "\n",
    "With histograms computed, we need to wrap the histograms in DescriptorElement instances and organize them for training the histogram classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train2_pos_ad_descriptors = set()\n",
    "train2_neg_ad_descriptors = set()\n",
    "for ad in train2_pos_ads:\n",
    "    train2_pos_ad_descriptors.add( ad_histogram_descriptor_index[ad] )\n",
    "for ad in train2_neg_ads:\n",
    "    train2_neg_ad_descriptors.add( ad_histogram_descriptor_index[ad] )\n",
    "        \n",
    "assert len(train2_pos_ad_descriptors) + len(train2_neg_ad_descriptors) == len(train2_ad2histogram), \\\n",
    "    \"Descriptors we sent to the classifier for training should add up with the number of histograms \" \\\n",
    "    \"we computed.\"\n",
    "assert len(train2_pos_ad_descriptors) + len(train2_neg_ad_descriptors) == ad_histogram_descriptor_index.count(), \\\n",
    "    \"Descriptors we sent to the classifier for training should add up with the number of histograms \" \\\n",
    "    \"we computed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(train2_pos_ad_descriptors), len(train2_neg_ad_descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Not setting a normalization value because we're already giving it L1 normalized histograms\n",
    "ad_classifier = LibSvmClassifier('ad_classifier.train2_classifier.model',\n",
    "                                 'ad_classifier.train2_classifier.labels')\n",
    "\n",
    "# Randomly subsampling positive/negative examples\n",
    "# TODO: Increase for final model\n",
    "n_pos = 24365\n",
    "n_neg = 8300\n",
    "\n",
    "pos_train_d = sorted(train2_pos_ad_descriptors, key=lambda d: d.uuid())\n",
    "neg_train_d = sorted(train2_neg_ad_descriptors, key=lambda d: d.uuid())\n",
    "numpy.random.seed(0)\n",
    "numpy.random.shuffle(pos_train_d)\n",
    "numpy.random.shuffle(neg_train_d)\n",
    "pos_train_d = pos_train_d[:n_pos]\n",
    "neg_train_d = neg_train_d[:n_neg]\n",
    "\n",
    "if not ad_classifier.has_model():\n",
    "    ad_classifier.train(positive=pos_train_d, negative=neg_train_d)\n",
    "else:\n",
    "    print \"Ad classifier already trained\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating Ad Histogram\n",
    "\n",
    "- Apply image classifier to train3 image SHA1s (`image_descr_index`)\n",
    "- Create sub-map of SHA1->ads (function `filter_sha2ads`)\n",
    "- Generate Ad histograms (function `generate_ad_histograms`)\n",
    "  and save to a DescriptorIndex\n",
    "- Save out ad truth CSV for validation script\n",
    "- Run validation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply image classifier to train3 image descriptors\n",
    "train3_pos_shas = cPickle.load(open('train3_pos_shas.pickle'))\n",
    "train3_neg_shas = cPickle.load(open('train3_neg_shas.pickle'))\n",
    "\n",
    "\n",
    "TRAIN3_IMG_CLASSIFICATIONS_CACHE = 'image_classifier.train3.classifications_cache.pickle'\n",
    "if os.path.isfile(TRAIN3_IMG_CLASSIFICATIONS_CACHE):\n",
    "    print \"Loading existing train3 image classification results\"\n",
    "    with open(TRAIN3_IMG_CLASSIFICATIONS_CACHE) as f:\n",
    "        train3_img_descr2classifications = cPickle.load(f)\n",
    "else:\n",
    "    train3_img_descriptors = image_descr_index.get_many_descriptors(train3_pos_shas | train3_neg_shas)\n",
    "    \n",
    "    train3_img_descr2classifications = image_classifier.classify_async(train3_img_descriptors,\n",
    "                                                                       c_file_factory,\n",
    "                                                                       use_multiprocessing=True,\n",
    "                                                                       ri=1.)\n",
    "    with open(TRAIN3_IMG_CLASSIFICATIONS_CACHE, 'w') as f:\n",
    "        cPickle.dump(train3_img_descr2classifications, f, -1)\n",
    "        \n",
    "# We should have a classification for all input descriptors\n",
    "assert len(train3_img_descr2classifications) == len(train3_pos_shas | train3_neg_shas), \\\n",
    "    \"There should be a classification for all input descriptors but wasn't\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create index of ad historgram descriptors\n",
    "train3_pos_ads = cPickle.load(open('train3_pos_ads.pickle'))\n",
    "train3_neg_ads = cPickle.load(open('train3_neg_ads.pickle'))\n",
    "train3_sha2ads = filter_sha2ads(train3_pos_shas | train3_neg_shas,\n",
    "                                train3_pos_ads | train3_neg_ads)\n",
    "assert len(train3_sha2ads) == len(train3_img_descr2classifications)\n",
    "\n",
    "train3_ad2histogram = generate_ad_histograms(train3_img_descr2classifications.itervalues(),\n",
    "                                              train3_sha2ads)\n",
    "with open('ad_classifier.train3_ad2histogram.pickle', 'w') as f:\n",
    "    cPickle.dump(train3_ad2histogram, f, -1)\n",
    "\n",
    "to_add = set()\n",
    "for ad in train3_ad2histogram:\n",
    "    if not ad_histogram_descriptor_index.has_descriptor(ad):\n",
    "        d = DescriptorMemoryElement('ad-histogram', ad)\n",
    "        d.set_vector(train3_ad2histogram[ad])\n",
    "        to_add.add(d)\n",
    "ad_histogram_descriptor_index.add_many_descriptors(to_add)\n",
    "del to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TRAIN3_VALIDATION_AD_TRUTH_CSV = 'ad_classifier.train3_validation.csv'\n",
    "with open(TRAIN3_VALIDATION_AD_TRUTH_CSV, 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for ad in train3_pos_ads:\n",
    "        writer.writerow([ad, 'positive'])\n",
    "    for ad in train3_neg_ads:\n",
    "        writer.writerow([ad, 'negative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the classifier validation script can be run to validate the train2-data trained ad classifier on train3 data:\n",
    "\n",
    "    classifier_model_validation.py -vc ad_classifier.train3_validation.json 2>&1 | tee ad_classifier.train3_validation.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster histograms\n",
    "\n",
    "- generate image classifications for train3 images (performed earlier during ad histograms validation)\n",
    "- generate ad histograms for train3 ads and classify (also done in previous step)\n",
    "- generate cluster histograms and train classifier\n",
    "- validate stack on test images/ads/clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_cluster_histograms(ad_classifications, ad2cluster, hist_size=20, target_class='positive'):\n",
    "    \"\"\"\n",
    "    Generate cluster histograms based on input iterable of ClassificationElements.\n",
    "    \n",
    "    :param ad_classifications: Iterable of ClassificationElement instances for all ads\n",
    "        classified.\n",
    "    :param ad2cluster: Mapping of ad descriptor UUID values (doc id) to the cluster it\n",
    "        is a child of\n",
    "    :param hist_size: The number of percentile partitions to make for the ad histograms.\n",
    "        This is literally the size of the histograms output.\n",
    "    :param target_class: The class probability to extract from input ClassificationElement\n",
    "        instances.\n",
    "    :return: Dict mapping of cluster ID to the its L1 normalized histogram of child ad\n",
    "        classification scores.\n",
    "    \"\"\"\n",
    "    cluster2ad_probs = collections.defaultdict(dict)\n",
    "    for c in ad_classifications:\n",
    "        ad = c.uuid\n",
    "        cluster = ad2cluster[ad]\n",
    "        cluster2ad_probs[cluster][ad] = c[target_class]\n",
    "            \n",
    "    cluster2histogram = {}\n",
    "    for cluster, ad_prob_map in cluster2ad_probs.iteritems():\n",
    "        h = make_histogram(ad_prob_map.values(), hist_size).astype(float)\n",
    "        if not h.sum():\n",
    "            raise RuntimeError(\"Zero-sum histogram for ad: %s (probs: %s)\" \n",
    "                               % (ad, ad_prob_map.values()))\n",
    "        h /= h.sum()\n",
    "        cluster2histogram[cluster] = h\n",
    "    \n",
    "    return cluster2histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reverse mapping of ads to their parent cluster. 1-to-1 relationship.\n",
    "pos_cluster2ads = cPickle.load(open('positive.cluster2ads.pickle'))\n",
    "neg_cluster2ads = cPickle.load(open('negative.cluster2ads.pickle'))\n",
    "ad2cluster = {}\n",
    "for c in pos_cluster2ads:\n",
    "    for ad in pos_cluster2ads[c]:\n",
    "        ad2cluster[ad] = c\n",
    "for c in neg_cluster2ads:\n",
    "    for ad in neg_cluster2ads[c]:\n",
    "        ad2cluster[ad] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate classifications for train3 ad histograms\n",
    "TRAIN3_AD_CLASSIFICATIONS_CACHE = 'ad_classifier.train3.classifications_cache.pickle'\n",
    "\n",
    "ad_c_file_factory = ClassificationElementFactory(FileClassificationElement, {\n",
    "        \"save_dir\": \"ad_classifier.classifications\",\n",
    "        \"subdir_split\": 10,\n",
    "    })\n",
    "\n",
    "if os.path.isfile(TRAIN3_AD_CLASSIFICATIONS_CACHE):\n",
    "    print \"Loading existing train3 ad classification results\"\n",
    "    train3_ad_descr2classifications = cPickle.load(open(TRAIN3_AD_CLASSIFICATIONS_CACHE))\n",
    "else:\n",
    "    ads_to_compute = train3_pos_ads | train3_neg_ads\n",
    "    train3_ad_descr2classifications = \\\n",
    "        ad_classifier.classify_async(ad_histogram_descriptor_index.get_many_descriptors(ads_to_compute),\n",
    "                                     ad_c_file_factory, \n",
    "                                     use_multiprocessing=True,\n",
    "                                     ri=1.)\n",
    "    \n",
    "    with open(TRAIN3_AD_CLASSIFICATIONS_CACHE, 'w') as f:\n",
    "        cPickle.dump(train3_ad_descr2classifications, f, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train3_cluster2histogram = generate_cluster_histograms(train3_ad_descr2classifications.itervalues(),\n",
    "                                                       ad2cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_histogram_descriptor_index = \\\n",
    "    MemoryDescriptorIndex('cluster_classifier.cluster_histogram_descriptor_index.pickle')\n",
    "\n",
    "to_add = set()\n",
    "for c in train3_cluster2histogram:\n",
    "    if not cluster_histogram_descriptor_index.has_descriptor(c):\n",
    "        d = d_mem_factory('cluster-histogram', c)\n",
    "        d.set_vector(train3_cluster2histogram[c])\n",
    "        to_add.add(d)\n",
    "print \"Add %d missing cluster histograms\" % len(to_add)\n",
    "cluster_histogram_descriptor_index.add_many_descriptors(to_add)\n",
    "del to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train cluster histogram classifier\n",
    "train3_pos_clusters = cPickle.load(open('train3_pos_clusters.pickle'))\n",
    "train3_neg_clusters = cPickle.load(open('train3_neg_clusters.pickle'))\n",
    "train3_pos_c_descriptors = set()\n",
    "train3_neg_c_descriptors = set()\n",
    "for c in train3_pos_clusters:\n",
    "    train3_pos_c_descriptors.add( cluster_histogram_descriptor_index[c] )\n",
    "for c in train3_neg_clusters:\n",
    "    train3_neg_c_descriptors.add( cluster_histogram_descriptor_index[c] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(train3_pos_c_descriptors), len(train3_neg_c_descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_classifier = LibSvmClassifier('cluster_classifier.train3.classifier.model',\n",
    "                                      'cluster_classifier.train3.classifier.label')\n",
    "\n",
    "if not cluster_classifier.has_model():\n",
    "    cluster_classifier.train(positive=train3_pos_c_descriptors, negative=train3_neg_c_descriptors)\n",
    "else:\n",
    "    print \"Cluster classifier already trained\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating cluster classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply image classifier to test image shas\n",
    "test_pos_shas = cPickle.load(open('test_pos_shas.pickle'))\n",
    "test_neg_shas = cPickle.load(open('test_neg_shas.pickle'))\n",
    "\n",
    "TEST_IMG_CLASSIFICATIONS_CACHE = 'image_classifier.test.classifications_cache.pickle'\n",
    "if os.path.isfile(TEST_IMG_CLASSIFICATIONS_CACHE):\n",
    "    print \"Loading existing test image classification results\"\n",
    "    with open(TEST_IMG_CLASSIFICATIONS_CACHE) as f:\n",
    "        test_img_descr2classifications = cPickle.load(f)\n",
    "else:\n",
    "    test_img_descriptors = image_descr_index.get_many_descriptors(test_pos_shas | test_neg_shas)\n",
    "    \n",
    "    test_img_descr2classifications = \\\n",
    "        image_classifier.classify_async( test_img_descriptors,\n",
    "                                         c_file_factory,\n",
    "                                         use_multiprocessing=True,\n",
    "                                         ri=1. )\n",
    "    with open(TEST_IMG_CLASSIFICATIONS_CACHE, 'w') as f:\n",
    "        cPickle.dump(test_img_descr2classifications, f, -1)\n",
    "        \n",
    "assert len(test_img_descr2classifications) == len(test_pos_shas | test_neg_shas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create ad histograms\n",
    "test_pos_ads = cPickle.load(open('test_pos_ads.pickle'))\n",
    "test_neg_ads = cPickle.load(open('test_neg_ads.pickle'))\n",
    "test_sha2ads = filter_sha2ads(test_pos_shas | test_neg_shas,\n",
    "                              test_pos_ads | test_neg_ads)\n",
    "assert len(test_sha2ads) == len(test_img_descr2classifications)\n",
    "\n",
    "test_ad2histogram = generate_ad_histograms(test_img_descr2classifications.itervalues(),\n",
    "                                           test_sha2ads)\n",
    "\n",
    "with open('ad_classifier.test_ad2histogram.pickle', 'w') as f:\n",
    "    cPickle.dump(test_ad2histogram, f, -1)\n",
    "\n",
    "# Add histograms as descriptors to index\n",
    "to_add = set()\n",
    "for ad in test_ad2histogram:\n",
    "    if not ad_histogram_descriptor_index.has_descriptor(ad):\n",
    "        d = DescriptorMemoryElement('ad-histogram', ad)\n",
    "        d.set_vector(test_ad2histogram[ad])\n",
    "        to_add.add(d)\n",
    "print \"Adding %d test ad histograms to index\" % len(to_add)\n",
    "ad_histogram_descriptor_index.add_many_descriptors(to_add)\n",
    "del to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Clasifying ad histograms\n",
    "TEST_AD_CLASSIFICATIONS_CACHE = 'ad_classifier.test.classifications_cache.pickle'\n",
    "if os.path.isfile(TEST_AD_CLASSIFICATIONS_CACHE):\n",
    "    with open(TEST_AD_CLASSIFICATIONS_CACHE) as f:\n",
    "        test_ad_descr2classifications = cPickle.load(f)\n",
    "else:\n",
    "    test_ad_descriptors = ad_histogram_descriptor_index.get_many_descriptors(test_pos_ads | test_neg_ads)\n",
    "    test_ad_descr2classifications = \\\n",
    "        ad_classifier.classify_async( test_ad_descriptors,\n",
    "                                      ad_c_file_factory,\n",
    "                                      use_multiprocessing=True,\n",
    "                                      ri=1.0 )\n",
    "    with open(TEST_AD_CLASSIFICATIONS_CACHE, 'w') as f:\n",
    "        cPickle.dump(test_ad_descr2classifications, f, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_histogram_descriptor_index = \\\n",
    "    MemoryDescriptorIndex('cluster_classifier.cluster_histogram_descriptor_index.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create cluster histograms\n",
    "test_cluster2histogram = generate_cluster_histograms(test_ad_descr2classifications.itervalues(),\n",
    "                                                     ad2cluster)\n",
    "\n",
    "test_pos_clusters = cPickle.load(open('test_pos_clusters.pickle'))\n",
    "test_neg_clusters = cPickle.load(open('test_neg_clusters.pickle'))\n",
    "assert len(test_cluster2histogram) == len(test_pos_clusters | test_neg_clusters)\n",
    "\n",
    "to_add = set()\n",
    "for c in test_cluster2histogram:\n",
    "    if not cluster_histogram_descriptor_index.has_descriptor(c):\n",
    "        d = DescriptorMemoryElement('cluster-histogram', c)\n",
    "        d.set_vector(test_cluster2histogram[c])\n",
    "        to_add.add(d)\n",
    "print \"Adding %d test cluster histograms to index\" % len(to_add)\n",
    "cluster_histogram_descriptor_index.add_many_descriptors(to_add)\n",
    "del to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Checking that the descriptor index has the histograms for the test set clusters\n",
    "for c in test_pos_clusters:\n",
    "    assert cluster_histogram_descriptor_index.has_descriptor(c)\n",
    "for c in test_neg_clusters:\n",
    "    assert cluster_histogram_descriptor_index.has_descriptor(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Createa CSV for cluster classifier validation against the test set\n",
    "TEST_VALIDATION_CLUSTER_TRUTH_CSV = 'cluster_classifier.test_validation.csv'\n",
    "with open(TEST_VALIDATION_CLUSTER_TRUTH_CSV, 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for c in test_pos_clusters:\n",
    "        writer.writerow([c, 'positive'])\n",
    "    for c in test_neg_clusters:\n",
    "        writer.writerow([c, 'negative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the classifier validation script can be run to validate the train3-data trained cluster classifier on test data:\n",
    "\n",
    "    classifier_model_validation.py -vc cluster_classifier.test_validation.json 2>&1 | tee cluster_classifier.test_validation.log\n",
    "    \n",
    "Normally, the ``classifier_model_validation.py`` script interprets UUID values as strings from the CSV file, but since we (wrongly) stored everything with the cluster ID as an integer, I changed the code to interpret the UUID field as an integer in the script temporarilly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

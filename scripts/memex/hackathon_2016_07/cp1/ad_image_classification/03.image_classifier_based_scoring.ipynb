{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Image Classifier\n",
    "\n",
    "We will use part of the training data provided to us, separated by high level [entity] clusters, to train the image classifier.  Due to the scale of the full dataset, a random subsample is taken.  See [this notebook block](http://localhost:8888/notebooks/02.train_tiered_classifiers.ipynb#Training-level-1---Image-classifier) for image classifier training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute evaluation image descriptors\n",
    "\n",
    "When we get the evaluation image data, we must compute descriptors for that data.  We will use the eval index (see the `common.descriptor_set.eval.json` config) and the common descriptor store (``common.descriptor_factory.json``).  Using the ``common.cmd.eval.config.json`` with the ``compute_many_descriptors.py`` script should be used, which is set to these locations.\n",
    "\n",
    "After descriptors are computed, we can proceed to scoring via the image classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using image classifier for scoring\n",
    "\n",
    "Here we will use the trained image classifer to score clustered ad images, pooling the maximum and average HT positive scores each ad and then cluster resulting in two score sets that we will \"submit\" for evaluation.\n",
    "\n",
    "Output must be in the form of an **ordered** json-lines file with each line having the structure:\n",
    "\n",
    "    {\"cluster_id\": \"...\", \"score\": <float>}\n",
    "\n",
    "Thus, we need the evaluation truth file in order to get the cluster ID ordering, which is also json-lines and of the form:\n",
    "\n",
    "    {\"cluster_id\": \"...\", \"class\": <int>}\n",
    "    ...\n",
    "    \n",
    "The evaluation script (for plotting the ROC curve) can be [found here](https://github.com/istresearch/qpr-summer-2016-eval/tree/master/CP1).\n",
    "\n",
    "The steps that need to be performed:\n",
    "1. Get images + cluster/ad/sha CSV\n",
    "- Compute descriptors for imagery provided\n",
    "- Load cluster/ad/sha maps after knowing what images were successfully described\n",
    "- Run classifier over descriptors computed\n",
    "- Determine ad/cluster scores via max/avg pooling\n",
    "- Output json-line files for scoring in evaluation script (linked above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize logging\n",
    "import logging\n",
    "from smqtk.utils.bin_utils import initialize_logging\n",
    "initialize_logging(logging.getLogger('smqtk'), logging.DEBUG)\n",
    "initialize_logging(logging.getLogger('__name__'), logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# File path parameters\n",
    "\n",
    "CMD_PROCESSED_CSV = 'eval.cmd.processed.csv'\n",
    "CLUSTER_ADS_IMAGES_CSV = 'eval.clusters_ads_images.csv'\n",
    "\n",
    "EVAL_IMAGE_CLASSIFICATIONS_CACHE = 'eval.image_classifications_cache.pickel'\n",
    "\n",
    "OUTPUT_MAX_SCORE_JL = 'eval.cluster_scores.max_pool.jl'\n",
    "OUTPUT_AVG_SCORE_JL = 'eval.cluster_scores.avg_pool.jl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from smqtk.algorithms.classifier.libsvm import LibSvmClassifier\n",
    "from smqtk.representation.classification_element.memory import MemoryClassificationElement\n",
    "from smqtk.representation.classification_element.file import FileClassificationElement\n",
    "from smqtk.representation import ClassificationElementFactory\n",
    "\n",
    "image_classifier = LibSvmClassifier('image_classifier.train1.classifier.model',\n",
    "                                    'image_classifier.train1.classifier.label',\n",
    "                                    normalize=2)\n",
    "c_file_factory = ClassificationElementFactory(FileClassificationElement,\n",
    "                                         {\n",
    "                                           \"save_dir\": \"image_classifier.classifications\",\n",
    "                                           \"subdir_split\": 10\n",
    "                                         })\n",
    "    \n",
    "from smqtk.representation import DescriptorSet\n",
    "from smqtk.utils.plugin import from_plugin_config\n",
    "with open('eval.test.cmd.json') as f:\n",
    "    descr_index = from_plugin_config(json.load(f)['descriptor_set'], DescriptorSet.get_impls())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "descr_index.count()  # should equal lines of eval.cmd .processed.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TESTING\n",
    "# Make up ground truth file from test-set clusters/ads/shas\n",
    "test_pos_clusters = cPickle.load(open('test_pos_clusters.pickle'))\n",
    "test_neg_clusters = cPickle.load(open('test_neg_clusters.pickle'))\n",
    "pos_cluster2ads = cPickle.load(open('positive.cluster2ads.pickle'))\n",
    "neg_cluster2ads = cPickle.load(open('negative.cluster2ads.pickle'))\n",
    "pos_ad2shas = cPickle.load(open('positive.ad2shas.pickle'))\n",
    "neg_ad2shas = cPickle.load(open('negative.ad2shas.pickle'))\n",
    "\n",
    "with open('eval.test.clusters_ads_images.csv', 'w') as csv_out:\n",
    "    writer = csv.writer(csv_out)\n",
    "    writer.writerow(['cluster', 'ad', 'sha1'])\n",
    "    for c in test_pos_clusters:\n",
    "        for ad in pos_cluster2ads[c]:\n",
    "            for sha in pos_ad2shas[ad]:\n",
    "                writer.writerow([c, ad, sha])\n",
    "    for c in test_neg_clusters:\n",
    "        for ad in neg_cluster2ads[c]:\n",
    "            for sha in neg_ad2shas[ad]:\n",
    "                writer.writerow([c, ad, sha])\n",
    "\n",
    "with open('eval.test.gt.jl', 'w') as f:\n",
    "    for c in sorted(test_pos_clusters | test_neg_clusters, key=lambda k: str(k)):\n",
    "        if c in test_pos_clusters:\n",
    "            f.write( json.dumps({'cluster_id': str(c), 'class': 1}) + '\\n' )\n",
    "        elif c in test_neg_clusters:\n",
    "            f.write( json.dumps({'cluster_id': str(c), 'class': 0}) + '\\n' )\n",
    "        else:\n",
    "            raise ValueError(\"Cluster %d not positive or negative?\" % c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step [3]\n",
    "\n",
    "# Load in successfully processed image shas\n",
    "# This is a result file from descriptor computation.\n",
    "with open(CMD_PROCESSED_CSV) as f:\n",
    "    computed_shas = {r[1] for r in csv.reader(f)}\n",
    "\n",
    "# Load cluster/ad/sha relationship maps, filtered by what was actually processed\n",
    "import collections\n",
    "cluster2ads = collections.defaultdict(set)\n",
    "cluster2shas = collections.defaultdict(set)\n",
    "ad2shas = collections.defaultdict(set)\n",
    "sha2ads = collections.defaultdict(set)\n",
    "with open(CLUSTER_ADS_IMAGES_CSV) as f:\n",
    "    reader = csv.reader(f)\n",
    "    for i, r in enumerate(reader):\n",
    "        if i == 0:\n",
    "            # skip header line\n",
    "            continue\n",
    "        c, ad, sha = r\n",
    "        if sha in computed_shas:\n",
    "            cluster2ads[c].add(ad)\n",
    "            cluster2shas[c].add(sha)\n",
    "            ad2shas[ad].add(sha)\n",
    "            sha2ads[sha].add(ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step [4]\n",
    "# Classify eval set images\n",
    "\n",
    "if os.path.isfile(EVAL_IMAGE_CLASSIFICATIONS_CACHE):\n",
    "    with open(EVAL_IMAGE_CLASSIFICATIONS_CACHE) as f:\n",
    "        image_descr2classifications = cPickle.load(f)\n",
    "else:\n",
    "    img_descriptors = descr_index.get_many_descriptors(set(sha2ads))\n",
    "    image_descr2classifications = image_classifier.classify_async(img_descriptors, \n",
    "                                                                  c_file_factory,\n",
    "                                                                  use_multiprocessing=True,\n",
    "                                                                  ri=1.0)\n",
    "    with open(EVAL_IMAGE_CLASSIFICATIONS_CACHE, 'w') as f:\n",
    "        cPickle.dump(image_descr2classifications, f, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step [5]\n",
    "print \"Collecting scores for SHA1s\"\n",
    "sha2score = {}\n",
    "for c in image_descr2classifications.itervalues():\n",
    "    sha2score[c.uuid] = c['positive']\n",
    "\n",
    "# select ads score from max and average of child image scores\n",
    "print \"Collecting scores for ads (MAX and AVG)\"\n",
    "import numpy\n",
    "ad2score_max = {}\n",
    "ad2score_avg = {}\n",
    "for ad, child_shas in ad2shas.iteritems():\n",
    "    scores = [sha2score[sha] for sha in child_shas]\n",
    "    ad2score_max[ad] = numpy.max(scores)\n",
    "    ad2score_avg[ad] = numpy.average(scores)\n",
    "\n",
    "# select cluster score from max and average of child ad scores\n",
    "print \"Collecting scores for ads (MAX and AVG)\"\n",
    "cluster2score_max = {}\n",
    "cluster2score_avg = {}\n",
    "for c, child_ads in cluster2ads.iteritems():\n",
    "    cluster2score_max[c] = numpy.max(    [ad2score_max[ad] for ad in child_ads])\n",
    "    cluster2score_avg[c] = numpy.average([ad2score_avg[ad] for ad in child_ads])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(cluster2score_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step [6]\n",
    "# Write out json-lines file in same order as GT file\n",
    "\n",
    "# The ordering we will save out json-lines (arbitrary?)\n",
    "cluster_id_order = sorted(cluster2score_avg.iterkeys())\n",
    "\n",
    "with open(OUTPUT_MAX_SCORE_JL, 'w') as f:\n",
    "    for c in cluster_id_order:\n",
    "        if c in cluster2score_max:\n",
    "            f.write( json.dumps({\"cluster_id\": c, \"score\": cluster2score_max[c]}) + '\\n' )\n",
    "        else:\n",
    "            # Due to a cluster having no child ads with imagery\n",
    "            f.write( json.dumps({\"cluster_id\": c, \"score\": 0.5}) + '\\n' )\n",
    "            \n",
    "with open(OUTPUT_AVG_SCORE_JL, 'w') as f:\n",
    "    for c in cluster_id_order:\n",
    "        if c in cluster2score_avg:\n",
    "            f.write( json.dumps({\"cluster_id\": c, \"score\": cluster2score_avg[c]}) + '\\n' )\n",
    "        else:\n",
    "            # Due to a cluster having no child ads with imagery\n",
    "            f.write( json.dumps({\"cluster_id\": c, \"score\": 0.5}) + '\\n' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "numpy.average(sha2score.values()), numpy.min(sha2score.values()), numpy.max(sha2score.values())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
